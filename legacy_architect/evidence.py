"""Evidence module - generate EVIDENCE.md documentation for refactoring."""

import os
from datetime import datetime
from typing import Dict, Any, Optional

from legacy_architect.artifacts import write_text, read_text, read_json, get_artifact_path


def generate_evidence_report(
    target_file: str,
    symbol: str,
    original_code: str,
    refactored_code: str,
    plan: Dict[str, Any],
    test_results: Dict[str, Any],
    iterations: int = 1
) -> str:
    """
    Generate a comprehensive EVIDENCE.md report.
    
    Args:
        target_file: Path to the refactored file
        symbol: Name of the refactored function
        original_code: The original legacy code
        refactored_code: The refactored code
        plan: The refactoring plan from Gemini
        test_results: Test results from both modes
        iterations: Number of fix iterations needed
    
    Returns:
        Path to the generated EVIDENCE.md file
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Build the report
    report = f"""# Legacy Architect - Refactoring Evidence Report

**Generated:** {timestamp}  
**Agent:** Legacy Architect (Gemini 3 Powered)  
**Model:** {os.environ.get("GEMINI_MODEL", "gemini-3-flash-preview")}

---

## ðŸ“‹ Executive Summary

This report documents the autonomous refactoring of `{symbol}` in `{target_file}` performed by the Legacy Architect agent using Gemini 3.

| Metric | Value |
|--------|-------|
| Target File | `{target_file}` |
| Target Symbol | `{symbol}` |
| Fix Iterations | {iterations} |
| Default Mode Tests | {"âœ… PASSED" if test_results.get("default_passed") else "âŒ FAILED"} |
| BILLING_V2 Mode Tests | {"âœ… PASSED" if test_results.get("v2_passed") else "âŒ FAILED"} |
| Total Tests | {test_results.get("total_tests", "N/A")} |

---

## ðŸŽ¯ Refactoring Plan

{_format_plan(plan)}

---

## ðŸ“ Code Changes

### Original Code (Before)

```python
{original_code}
```

### Refactored Code (After)

```python
{refactored_code}
```

---

## ðŸ” Key Improvements

{_extract_improvements(plan, original_code, refactored_code)}

---

## âœ… Test Verification

### Default Mode (No Feature Flag)

{_format_test_result("default", test_results)}

### BILLING_V2 Mode (Feature Flag Enabled)

{_format_test_result("v2", test_results)}

### Dual-Mode Equivalence

The refactored code produces **identical outputs** to the original implementation across all {test_results.get("total_tests", "N/A")} test cases, verifying that this is a behavior-preserving refactor.

---

## ðŸ“Š Impact Analysis

{_format_impact_analysis()}

---

## ðŸ›¡ï¸ Safety Measures

The following safety measures were employed during this refactoring:

1. **Characterization Tests**: {test_results.get("total_tests", "N/A")} tests were generated to capture the exact behavior of the original code
2. **Dual-Mode Testing**: Tests ran in both default and BILLING_V2 modes to ensure equivalence
3. **Automatic Backup**: Original file was backed up before any modifications
4. **Syntax Validation**: All generated code was validated for Python syntax before applying
5. **Iterative Fixing**: Up to {iterations} iterations were used to fix any test failures

---

## ðŸ“ Artifacts Generated

| File | Description |
|------|-------------|
| `artifacts/plan.json` | Detailed refactoring plan from Gemini |
| `artifacts/impact.json` | Symbol usage analysis across codebase |
| `artifacts/diff.patch` | Unified diff of code changes |
| `artifacts/test_default.log` | Test output from default mode |
| `artifacts/test_flag.log` | Test output from BILLING_V2 mode |
| `artifacts/EVIDENCE.md` | This report |

---

## ðŸ¤– Gemini Integration

This refactoring was performed autonomously by the Legacy Architect agent using:

- **Gemini 3 Flash Preview** for code analysis and generation
- **Multi-step agentic workflow**: Plan â†’ Patch â†’ Test â†’ Fix â†’ Evidence
- **Context-aware prompting** with original code, call sites, and test failures
- **Iterative refinement** to achieve 100% test compatibility

---

## ðŸ“œ Conclusion

The refactoring of `{symbol}` was **{"successful" if test_results.get("default_passed") and test_results.get("v2_passed") else "unsuccessful"}**.

{"All characterization tests pass in both modes, confirming that the refactored code maintains identical behavior to the original implementation while improving code quality, readability, and maintainability." if test_results.get("default_passed") and test_results.get("v2_passed") else "Some tests failed. Please review the test logs for details."}

---

*Report generated by Legacy Architect v1.0*
"""
    
    # Write the report
    write_text("EVIDENCE.md", report)
    
    return get_artifact_path("EVIDENCE.md")


def _format_plan(plan: Dict[str, Any]) -> str:
    """Format the refactoring plan for the report."""
    if not plan:
        return "No plan available"
    
    sections = []
    
    if "summary" in plan:
        sections.append(f"**Summary:** {plan['summary']}")
    
    if "issues" in plan and plan["issues"]:
        sections.append("\n**Issues Identified:**")
        for issue in plan["issues"]:
            sections.append(f"- {issue}")
    
    if "improvements" in plan and plan["improvements"]:
        sections.append("\n**Planned Improvements:**")
        for imp in plan["improvements"]:
            sections.append(f"- {imp}")
    
    if "constants" in plan and plan["constants"]:
        sections.append("\n**Constants to Extract:**")
        for const in plan["constants"]:
            sections.append(f"- {const}")
    
    if "helper_functions" in plan and plan["helper_functions"]:
        sections.append("\n**Helper Functions:**")
        for helper in plan["helper_functions"]:
            sections.append(f"- {helper}")
    
    if "risks" in plan and plan["risks"]:
        sections.append("\n**Identified Risks:**")
        for risk in plan["risks"]:
            sections.append(f"- âš ï¸ {risk}")
    
    return "\n".join(sections) if sections else "*Plan details not available*"


def _extract_improvements(plan: Dict, original: str, refactored: str) -> str:
    """Extract and format key improvements made."""
    improvements = []
    
    # Check for docstrings added
    if '"""' in refactored and '"""' not in original:
        improvements.append("âœ… **Added docstrings** for better documentation")
    
    # Check for type hints
    if "->" in refactored and "->" not in original:
        improvements.append("âœ… **Added type hints** for better IDE support")
    
    # Check for constants (UPPER_CASE variables)
    import re
    constants = re.findall(r'^[A-Z][A-Z_0-9]+ = ', refactored, re.MULTILINE)
    if constants:
        improvements.append(f"âœ… **Extracted {len(constants)} constants** to improve readability")
    
    # Check for helper functions
    original_funcs = len(re.findall(r'^def ', original, re.MULTILINE))
    refactored_funcs = len(re.findall(r'^def ', refactored, re.MULTILINE))
    if refactored_funcs > original_funcs:
        improvements.append(f"âœ… **Created {refactored_funcs - original_funcs} helper functions** for modularity")
    
    # Line count comparison
    original_lines = len(original.strip().split('\n'))
    refactored_lines = len(refactored.strip().split('\n'))
    if refactored_lines != original_lines:
        change = "increased" if refactored_lines > original_lines else "decreased"
        improvements.append(f"ðŸ“Š Line count {change} from {original_lines} to {refactored_lines}")
    
    # From plan
    if plan and "improvements" in plan:
        for imp in plan["improvements"][:3]:  # First 3 from plan
            improvements.append(f"âœ… {imp}")
    
    return "\n".join(improvements) if improvements else "*No specific improvements documented*"


def _format_test_result(mode: str, results: Dict) -> str:
    """Format test results for a specific mode."""
    if mode == "default":
        passed = results.get("default_passed", False)
        count = results.get("default_count", {})
    else:
        passed = results.get("v2_passed", False)
        count = results.get("v2_count", {})
    
    status = "âœ… **PASSED**" if passed else "âŒ **FAILED**"
    
    return f"""
- **Status:** {status}
- **Passed:** {count.get("passed", "N/A")}
- **Failed:** {count.get("failed", 0)}
- **Errors:** {count.get("errors", 0)}
"""


def _format_impact_analysis() -> str:
    """Format impact analysis from the impact.json artifact."""
    impact = read_json("impact.json")
    
    if not impact:
        return "*Impact analysis not available*"
    
    summary = impact.get("summary", {})
    
    text = f"""
| Metric | Count |
|--------|-------|
| Files Affected | {summary.get("total_files", "N/A")} |
| Total Usages | {summary.get("total_usages", "N/A")} |
| Call Sites | {summary.get("call_sites", "N/A")} |
| Test Files | {summary.get("test_files", "N/A")} |
"""
    
    # List call sites if available
    call_sites = impact.get("call_sites", [])
    if call_sites:
        text += "\n**Call Sites:**\n"
        for site in call_sites[:5]:  # First 5
            text += f"- `{site.get('file', 'Unknown')}` (lines: {site.get('lines', [])})\n"
    
    return text


def quick_evidence(
    symbol: str,
    test_passed: bool,
    iterations: int
) -> str:
    """
    Generate a quick evidence summary without full details.
    
    Useful for progress updates during the agent run.
    """
    status = "âœ… SUCCESS" if test_passed else "âŒ FAILED"
    
    return f"""
## Quick Evidence Summary

- **Symbol:** {symbol}
- **Status:** {status}
- **Iterations:** {iterations}
- **Timestamp:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""


if __name__ == "__main__":
    print("Testing evidence module...")
    
    # Create a sample report
    sample_original = '''def calc(items):
    t = 0
    for i in items:
        t += i["p"] * i["q"]
    return t'''
    
    sample_refactored = '''"""Calculate total price."""

def calculate_total(items: list) -> float:
    """
    Calculate the total price of all items.
    
    Args:
        items: List of item dicts with 'price' and 'qty' keys
    
    Returns:
        Total price as float
    """
    total = 0.0
    for item in items:
        total += item["price"] * item["qty"]
    return total'''
    
    sample_plan = {
        "summary": "Improve readability and add documentation",
        "issues": ["Poor variable names", "No docstring", "No type hints"],
        "improvements": ["Rename variables", "Add docstring", "Add type hints"],
    }
    
    sample_results = {
        "default_passed": True,
        "v2_passed": True,
        "total_tests": 24,
        "default_count": {"passed": 24, "failed": 0},
        "v2_count": {"passed": 24, "failed": 0},
    }
    
    report_path = generate_evidence_report(
        target_file="app/legacy/billing.py",
        symbol="calculate_total",
        original_code=sample_original,
        refactored_code=sample_refactored,
        plan=sample_plan,
        test_results=sample_results,
        iterations=1
    )
    
    print(f"âœ… Evidence report generated: {report_path}")
    
    # Show first few lines
    report = read_text("EVIDENCE.md")
    if report:
        print("\n--- First 20 lines ---")
        print("\n".join(report.split("\n")[:20]))
